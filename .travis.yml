language: python
sudo: false
dist: trusty
services:
- docker
cache:
  directories:
  - "$HOME/.cache/pip"
addons:
  apt:
    packages:
      - libboost-all-dev
      - libxml2-dev
      - libsnappy-dev
_install: &_install
  - gimme 1.8
  - source ~/.gimme/envs/latest.env
  - pip install --upgrade pip
  - (cd /tmp && wget "https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz" -O spark-2.2.0-bin-hadoop2.7.tgz; tar -C ~/ -xvzf spark-2.2.0-bin-hadoop2.7.tgz)
  - export SPARK_HOME=~/spark-2.2.0-bin-hadoop2.7
  - export PATH=$PATH:$SPARK_HOME/bin
  - (cd "$(python -c 'import pyspark; print(pyspark.__path__[0])')/jars"; wget http://search.maven.org/remotecontent?filepath=tech/sourced/engine/0.2.0/engine-0.2.0.jar -O engine.jar)
  - pip install -r requirements.txt codecov
  - pip install -e .
_coverage: &_coverage
  - SCRIPT="coverage run --concurrency=multiprocessing -m unittest discover && coverage combine"
matrix:
  include:
    - python: 3.4
      env: *_coverage
      install: *_install
    - python: 3.5
      env: *_coverage
      install: *_install
    - python: 3.6
      env: SCRIPT="pep8 --max-line-length=99 ."
      install: pip install pep8
    - python: 3.6
      env: *_coverage
      install: *_install
      after_success:
        - codecov
  fast_finish: true
before_script:
- docker run -d --privileged -p 9432:9432 --name bblfshd bblfsh/bblfshd
- docker exec -it bblfshd bblfshctl driver install --all
script:
- (eval "$SCRIPT")
notifications:
  email: false
