language: python
sudo: false
dist: trusty
services:
- docker
cache:
  directories:
  - "$HOME/.cache/pip"
stages:
  - test
  - name: deploy
    # require any tag name to deploy
    if: tag =~ .*
_install: &_install
  - gimme 1.8
  - source ~/.gimme/envs/latest.env
  - pip install --upgrade pip
  - export HADOOP_VERSION=2.7
  - export SPARK_VERSION=2.2.0
  - export SPARK_NAME=spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION
  - (cd /tmp && wget "https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=spark/spark-$SPARK_VERSION/$SPARK_NAME.tgz" -O $SPARK_NAME.tgz; tar -C ~/ -xvzf $SPARK_NAME.tgz)
  - export SPARK_HOME=~/$SPARK_NAME
  - export PATH=$PATH:$SPARK_HOME/bin
  - export PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip
  - pip install tensorflow keras
  - pip install -r requirements.txt codecov
  - pip install -e .
_coverage: &_coverage
  - SCRIPT="coverage run --concurrency=multiprocessing -m unittest discover && coverage combine"
_deploy: &_deploy
  provider: script
  script: twine upload dist/*py3-none-any* -u $PYPI_LOGIN -p $PYPI_PASS
  skip_cleanup: true
  on:
    tags: true
matrix:
  include:
    - python: 3.4
      env: *_coverage
      install: *_install
    - python: 3.5
      env: *_coverage
      install: *_install
    - python: 3.6
      env: SCRIPT="pep8 --max-line-length=99 ."
      install: pip install pep8
    - python: 3.6
      env: *_coverage
      install: *_install
      after_success:
        - codecov
    - stage: deploy
      python: 3.4
      install:
        - pip3 install --upgrade pip
        - pip3 install twine
      before_script: skip
      script:
        - python3 setup.py bdist_wheel
      deploy: *_deploy
  fast_finish: true
before_script:
  - docker run -d --privileged -p 9432:9432 --name bblfshd bblfsh/bblfshd
  - docker exec -it bblfshd bblfshctl driver install python bblfsh/python-driver
script:
  - (eval "$SCRIPT")
notifications:
  email: false
